// --- Robots.txt ---
const robots = `# ===============================
# Dareloom Hub - robots.txt (SEO + AI Protection)
# ===============================
# Place this file at: https://dareloom.fun/robots.txt

# -------------------------------
# 1) Block AI & Scraper Bots (best-effort)
# -------------------------------
User-agent: GPTBot
Disallow: /

User-agent: ClaudeBot
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: Diffbot
Disallow: /

User-agent: OmgiliBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: ChatGPT
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: meta-externalagent
Disallow: /

# -------------------------------
# 2) Major Search Engines (SEO Optimized)
# -------------------------------
User-agent: Googlebot
Allow: /
Disallow: /admin/
Disallow: /private/

User-agent: Bingbot
Allow: /

User-agent: Yandex
Allow: /

# -------------------------------
# 3) Default Rules (for all other bots)
# -------------------------------
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/
Disallow: /node_modules/
Disallow: /scripts/
Disallow: /private/
Disallow: /temp/
Disallow: /*?*sort=*
Disallow: /*?*filter=*
Disallow: /*?*session=*
Disallow: /*?*ref=*
Disallow: /*?*utm_*

# Optional crawl rate limiter
Crawl-delay: 5

# -------------------------------
# Sitemap Location
# -------------------------------
Sitemap: ${BASE_URL}/sitemap.xml
Sitemap: ${BASE_URL}/sitemap-video.xml
Sitemap: ${BASE_URL}/sitemap-seo.xml
# ===============================
# End of File
# ===============================`;

fs.writeFileSync(ROBOTS_PATH, robots);
console.log("âœ… robots.txt created (AI-block + SEO rules)");
