// --- Robots.txt Generation Script ---
const fs = require('fs'); // Make sure you have this line if it's a Node.js script

const BASE_URL = "https://dareloom.fun"; // <<<--- YAHAN APNI WEBSITE KA URL DAALEIN
const ROBOTS_PATH = "adulthub/robots.txt"; // <<<--- YAHAN FILE KA SAHI PATH DAALEIN

const robots = `# ===============================
# Dareloom Hub - robots.txt (SEO + AI Protection)
# Generated on: ${new Date().toISOString()}
# ===============================
# Place this file at: https://dareloom.fun/robots.txt

# -------------------------------
# 1) Block AI & Scraper Bots (best-effort)
# -------------------------------
User-agent: GPTBot
Disallow: /

User-agent: ClaudeBot
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: Diffbot
Disallow: /

User-agent: OmgiliBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: Google-Extended
Disallow: /

# -------------------------------
# 2) Major Search Engines (SEO Optimized)
# -------------------------------
User-agent: Googlebot
Allow: /
Disallow: /admin/
Disallow: /private/

User-agent: Bingbot
Allow: /
Disallow: /admin/
Disallow: /private/

User-agent: Yandex
Allow: /
Disallow: /admin/
Disallow: /private/

# -------------------------------
# 3) Default Rules (for all other bots)
# -------------------------------
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/
Disallow: /node_modules/
Disallow: /scripts/
Disallow: /private/
Disallow: /temp/
Disallow: /*?*sort=*
Disallow: /*?*filter=*
Disallow: /*?*session=*
Disallow: /*?*ref=*
Disallow: /*?*utm_*

# Crawl-delay is not respected by Google, but useful for others.
# Value is in seconds. A moderate value is better.
Crawl-delay: 2

# -------------------------------
# Sitemap Location
# -------------------------------
Sitemap: ${BASE_URL}/sitemap.xml
Sitemap: ${BASE_URL}/sitemap-video.xml
Sitemap: ${BASE_URL}/sitemap-seo.xml
# ===============================
# End of File
# ===============================`;

// This will write the file to the specified path
fs.writeFileSync(ROBOTS_PATH, robots.trim());

console.log(`âœ… robots.txt created successfully at ${ROBOTS_PATH}`);
